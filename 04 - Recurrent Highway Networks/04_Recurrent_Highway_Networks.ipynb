{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 - Recurrent Highway Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "DqSKYspRsb27",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 04 - Recurrent Highway Networks\n",
        "By Jan Christian Blaise B. Cruz"
      ]
    },
    {
      "metadata": {
        "id": "Hmz6-ggi6HYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the past notebooks, we've been building up on architectures that had a Long-Short Term Memory (Hochreiter & Schmidhuber, 1997) as the model backbone. In this notebook, we'll explore the use of a different recurrence strategy and a new architecture, called Recurrent Highway Networks (Zilly et al., 2017).\n",
        "\n",
        "Let's start with the imports."
      ]
    },
    {
      "metadata": {
        "id": "fVmQ28x9_5o5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchtext.data import Field, BPTTIterator\n",
        "from torchtext.datasets import PennTreebank\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLR1_Eg9p8rq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll use the Torchtext library to load and prepare our dataset. Again, we'll use the Penn Treebank, splitting them into train, valid, and test sets, tokenizing them with SpaCy's english tokenizer, and building the vocabulary with words of frequency of at least 2."
      ]
    },
    {
      "metadata": {
        "id": "LPGtGRnKEm2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4562e66e-8678-430f-9f9c-19cf33dad6e2"
      },
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "spacy_en = spacy.load('en')\n",
        "def tokenize(s):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(s)]\n",
        "\n",
        "# Prepare a field and get the data\n",
        "TEXT = Field(lower=True, tokenize=tokenize)\n",
        "train_data, valid_data, test_data = PennTreebank.splits(TEXT)\n",
        "\n",
        "# Build the vocabulary\n",
        "TEXT.build_vocab(train_data, min_freq=2)\n",
        "print(\"Vocab size: {}\".format(len(TEXT.vocab)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 9702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lV-qUGImqNPX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll batch them, again using the ```BPTTIterator``` following common conventions on batch size and sequence length."
      ]
    },
    {
      "metadata": {
        "id": "Z6auxsasEnv9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 40\n",
        "bptt_len = 35\n",
        "\n",
        "train_loader, valid_loader, test_loader = BPTTIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=batch_size, bptt_len=bptt_len, \n",
        "    device=device, \n",
        "    repeat=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Tk9c2inqrBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For convenience, we'll define recurrent dropout like we did in the previous notebooks."
      ]
    },
    {
      "metadata": {
        "id": "ECDP0mwaE1in",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Credits to the contributors at fast.ai\n",
        "def dropout_mask(x, sz, p):\n",
        "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
        "  \n",
        "class RNNDropout(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p=p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.p == 0.: \n",
        "          return x\n",
        "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
        "        return x * m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYCs_H4Z01Iu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Motivations\n"
      ]
    },
    {
      "metadata": {
        "id": "FHP7NpfIE1-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before we dive in to **Recurrent Highway Networks**, we first set some movitvations.\n",
        "\n",
        "### Residual Layers\n",
        "\n",
        "A **Residual connection** (He et al., 2015) in a neural network is a mechanism that mitigates vanishing gradients via \"skip connections\" that allow smooth gradient flow. Using residual connections aid in training of deep neural networks and this has been shown over time with empirical results. Given an input vector $x \\in \\mathbb{R}^n$, the output for a certain layer in a residual neural network is given by:\n",
        "\n",
        "$$y = f(W x + b) + x$$\n",
        "\n",
        "where $W$ and $b$ are the weight matrix and the bias vector of the layer and $f$ is a nonlinear activation function. Residual layers have success stories in many applications and have found homes in state-of-the-art architectures, such as the ResNet (He et al., 2015) series in computer vision, and BERT (Devlin et al., 2019) in natural language processing. "
      ]
    },
    {
      "metadata": {
        "id": "5R7NDo8vJ50U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Highway Layers\n",
        "\n",
        "A modification to the residual layer is the **Highway Layer** (Srivastava et al.,  2015a). Inspired by gating mechanisms in the Long-Short Term Memory (Hochreiter & Schmidhuber, 1997), the highway layer uses gates to control how much information to pass and how much information to retain from the skip connection via learned weights.\n",
        "\n",
        "Given $h = H(x, W_H)$, $t = T(x, W_T)$, and $c = C(x, W_C)$ where $h$, $t$, and $c$ are the results of nonlinear transforms $H$, $T$, and $C$ with associated weight matrices $W_{H, T, C}$ and biases $b_{H, T, C}$, the output $y$ of a highway layer is computed as:\n",
        "\n",
        "$$y = h \\odot t + x \\odot c$$\n",
        "\n",
        "Where $\\odot$ is the hadamard (elementwise) product operation. In practice, $H$ often uses the $tanh$ nonlinearity, and the $T$ and $C$ use the sigmoid ($\\sigma$) nonlinearity.\n",
        "\n",
        "$H$ can often be thought of as the \"main non-linear transform\" of the input $x$. $T$ and $C$ act as gates, controling from a range of $[0, 1]$ how much of the transformed input and the original input are to be carried over. In practice, a suggestion from the Highway networks paper is to couple the $C$ gate to the output of the $T$ gate by setting $C(\\cdot) = 1 - T(\\cdot)$. This reduces the parameters to optimize and could prevent an unbounded blow-up of states, which makes optimization smoother. However, this imposes a modeling bias, which could prove suboptimal for certain tasks (Greff et al., 2015; Jozefowicz et al., 2015)."
      ]
    },
    {
      "metadata": {
        "id": "aYiWPvNqKBe_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Recurrrent Highways\n",
        "A **Recurrent Highway** (Zilly et al., 2017) adapts the idea of a highway layer to include a recurrence mechanism, acting as a drop-in replacement for LSTMs or other gated-RNNs for a variety of sequence modeling tasks. An improvement that recurrent highways have is a timestep-to-timestep transition larger than one, as opposed to common gated-RNNs.\n",
        "\n",
        "Recall that a general RNN transition given $s$ timesteps is in the form:\n",
        "\n",
        "$$y^{[s]} = f(Wx^{[s]} + Ry^{[s - 1]} + b)$$\n",
        "\n",
        "A one-depth Recurrent Highway Network transition is given by:\n",
        "\n",
        "$$ h^{[s]} = tanh(W_{H}x^{[s]} + R_{H}y^{[s - 1]} + b) $$\n",
        "$$ t^{[s]} = \\sigma(W_{T}x^{[s]} + R_{T}y^{[s - 1]} + b) $$\n",
        "$$ c^{[s]} = 1 - t^{[s]} $$\n",
        "$$ y^{[s]} = h^{[s]} \\odot t^{[s]} + y^{[s - 1]} \\odot c^{[s]} $$\n",
        "\n",
        "By stacking multiple recurrent highways on top of each other, we could achieve a larger timestep-to-timestep transition. Given $L$ layers,  $l = \\{1, 2, ..., L\\}$ \"ticks\" in each timestep, and $s_l$ as the intermediate output between stacked layers, the recurrence can be expanded to:\n",
        "\n",
        "$$h^{[s]}_l = tanh(W_Hx^{[s]}\\mathcal{I}_{\\{l=1\\}} + R_{H_l}s^{[s]}_{l-1} + b_{H_l})$$\n",
        "$$t^{[s]}_l = \\sigma(W_Tx^{[s]}\\mathcal{I}_{\\{l=1\\}} + R_{T_l}s^{[s]}_{l-1} + b_{T_l})$$\n",
        "$$c^{[s]}_l = \\sigma(W_Cx^{[s]}\\mathcal{I}_{\\{l=1\\}} + R_{C_l}s^{[s]}_{l-1} + b_{C_l})$$\n",
        "$$s^{[t]}_0 = y^{[t-1]}$$\n",
        "$$s^{[t]}_l = h^{[s]}_l \\odot t^{[s]}_l + s^{[t]}_l-1 \\odot c^{[s]}_l$$\n",
        "\n",
        "where $\\mathcal{I}$ is the indicator function. Like in the standard highway layer and the one-depth recurrent highway, the $C$ and $T$ gates can be coupled setting $c^{[s]}_l = 1 - t^{[s]}_l$, reducing the numer of parameters to optimize. We can introduce recurrent dropout (Semeniuta et al., 2014) on $t$ as a one hyperparameter for all layers. The recurrent highway can be used as a drop-in replacement to any gated-RNN cell in any sequence-modeling architecture.\n",
        "\n",
        "We construct our highway layer block such that it can be used as a standard highway layer or can be stacked in a recurrent highway layer. The ```first``` argument should set to ```True``` when stacking (see indicator function when $l = 1$ in the equations.) We can set the option to ```couple``` $C$ and $T$ as well. We also use recurrent dropout on $t$ as needed."
      ]
    },
    {
      "metadata": {
        "id": "MEFufMSjEp28",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class HighwayBlock(nn.Module):\n",
        "    \"\"\" Highway Layer Block. Can be used as a highway layer or stacked into a recurrent highway. \"\"\"\n",
        "    def __init__(self, in_features, out_features, first=False, couple=False, dropout=0.0):\n",
        "        super(HighwayBlock, self).__init__()\n",
        "        self.first = first\n",
        "        self.couple = couple\n",
        "        if first:\n",
        "            self.W_H = nn.Linear(in_features, out_features, bias=False)\n",
        "            self.W_T = nn.Linear(in_features, out_features, bias=False)\n",
        "            if not couple: self.W_C = nn.Linear(in_features, out_features, bias=False)\n",
        "        self.R_H = nn.Linear(in_features, out_features)\n",
        "        self.R_T = nn.Linear(in_features, out_features)\n",
        "        if not couple: self.R_C = nn.Linear(in_features, out_features)\n",
        "        self.dropout = RNNDropout(dropout)\n",
        "    \n",
        "    def forward(self, x, s):\n",
        "        if self.first:\n",
        "            h = torch.tanh(self.W_H(x) + self.R_H(x))\n",
        "            t = torch.sigmoid(self.W_T(x) + self.R_T(x))\n",
        "            if self.couple:\n",
        "                c = 1 - t\n",
        "            else:\n",
        "                c = torch.sigmoid(self.W_C(x) + self.R_C(x))\n",
        "        else:\n",
        "            h = torch.tanh(self.R_H(x))\n",
        "            t = torch.sigmoid(self.R_T(x))\n",
        "            if self.couple:\n",
        "                c = 1 - t\n",
        "            else:\n",
        "                c = torch.sigmoid(self.R_C(x))\n",
        "        t = self.dropout(t.unsqueeze(0)).squeeze(0)\n",
        "        \n",
        "        return h * t + s * c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mGhsfZOj71R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The recurrent highway is simply a wrapper for a stacked highway block. We overload ```forward``` to apply the recurrence. In practice, this layer can be used as a direct replacement for a gated-RNN layer, and functions quite similar to a Gated Recurrent Unit (Cho et al., 2014) albeit with a larger timestep-to-timestep transition.\n",
        "\n",
        "Like other RNN layers, it expects inputs of dimensions ```[seq_len, bs, inp_dim]```. We definte it like so."
      ]
    },
    {
      "metadata": {
        "id": "M-ZlhJcgj7Q5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RecurrentHighway(nn.Module):\n",
        "    \"\"\"Recurrent Highway Layer. Stacks highway blocks with a recurrence mechanism. Replaces LSTM/GRU.\"\"\"\n",
        "    def __init__(self, in_features, out_features, recurrence_depth=5, couple=False, dropout=0):\n",
        "        super(RecurrentHighway, self).__init__()\n",
        "        self.highways = [HighwayBlock(in_features, out_features, \n",
        "                                      first=True if l == 0 else False, \n",
        "                                      couple=couple, dropout=dropout) for l in range(recurrence_depth)]\n",
        "        self.highways = nn.ModuleList(self.highways)\n",
        "        self.recurrence_depth=recurrence_depth\n",
        "        self.hidden_dim = out_features\n",
        "    \n",
        "    def forward(self, inp, hidden):\n",
        "        # expects input dimensions [seq_len, bs, inp_dim]\n",
        "        outputs = []\n",
        "        for x in inp:\n",
        "            for block in self.highways:\n",
        "                hidden = block(x, hidden)\n",
        "                \n",
        "            outputs.append(hidden)\n",
        "        outputs = torch.stack(outputs)\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDLqC34Tne_A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Highway Language Model"
      ]
    },
    {
      "metadata": {
        "id": "055KqbqFkXdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then adapt this to a language model architecture. We explicitly separate multiple layers of recurrent highways so that we can use \"hidden-to-hidden\" dropout as a regularization tactic. We also add in the option to tie the weights."
      ]
    },
    {
      "metadata": {
        "id": "mH0udzxnErjW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RHNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_sz, embedding_dim, hidden_dim, recurrence_depth=1, \n",
        "                 num_layers=1, hidden_dp=0.65, recur_dp=0.3, tie_weights=True, couple=False):\n",
        "        super(RHNLanguageModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_sz, embedding_dim)\n",
        "        self.rnns = [RecurrentHighway(embedding_dim if l == 0 else hidden_dim, \n",
        "                                      (hidden_dim if l != num_layers - 1 else embedding_dim) if tie_weights else hidden_dim, \n",
        "                                      recurrence_depth=recurrence_depth, couple=couple, dropout=recur_dp) for l in range(num_layers)]\n",
        "        self.rnns = nn.ModuleList(self.rnns)\n",
        "        self.fc1 = nn.Linear(embedding_dim if tie_weights else hidden_dim, vocab_sz)\n",
        "        self.hidden_dropout = RNNDropout(hidden_dp)\n",
        "        \n",
        "        if tie_weights:\n",
        "            self.fc1.weight = self.embedding.weight\n",
        "        \n",
        "    def init_hidden(self, bs):\n",
        "        # Returns a list of zeroed hidden states of dimensions [bs, hidden_dim]\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = [weight.new(bs, rnn.hidden_dim).zero_() for rnn in self.rnns]\n",
        "        \n",
        "        return hidden\n",
        "    \n",
        "    def forward(self, x):\n",
        "        bptt_len, bs = x.shape\n",
        "        vocab_sz = self.embedding.num_embeddings\n",
        "        \n",
        "        out = self.embedding(x) \n",
        "        \n",
        "        hiddens = self.init_hidden(bs)\n",
        "        for i, rnn in enumerate(self.rnns):\n",
        "          out, hidden = rnn(out, hiddens[i])\n",
        "          out = self.hidden_dropout(out)\n",
        "\n",
        "        out = self.fc1(out.flatten(0, 1))\n",
        "        out = out.view(bptt_len, bs, vocab_sz)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Ezeo3eKoB6F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We instantiate a model and initialize its parameters between $[-0.01, 0.01]$. We'll optimize the network via the Adam optimizer (KIngma & Ba, 2014) using a cross-entropy objective. We'll use two layers with a hidden size of $650$ units. We'll tie the encoder and projection weights. This is reminiscent of the architecture used by Press & Wolf (2016), which in turn was based on the setup of Zaremba et al. (2014).\n",
        "\n",
        "We choose to use a recurrence depth of 5 and couple $C$ and $T$ to reduce parameters. We'll apply $0.3$ recurrent dropout to $t$ and $0.65$ to the \"hidden-to-hidden\" connections between the recurrent layers."
      ]
    },
    {
      "metadata": {
        "id": "C0Ad2LRqEtAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23386d6b-bb1e-4113-df50-fa80bcfd41af"
      },
      "cell_type": "code",
      "source": [
        "model = RHNLanguageModel(vocab_sz=len(TEXT.vocab), embedding_dim=300, hidden_dim=650, \n",
        "                         recurrence_depth=5, num_layers=2, recur_dp=0.3, hidden_dp=0.65, \n",
        "                         tie_weights=True, couple=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.01, 0.01)\n",
        "        \n",
        "model.apply(init_weights)            \n",
        "print(\"The model has {:,} trainable parameters\".format(count_parameters(model)))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 7,609,802 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9674Qb8MpCsP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We train it for 40 epochs, saving the model as validation loss decreases."
      ]
    },
    {
      "metadata": {
        "id": "uJ0FmlvVEu12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2737
        },
        "outputId": "a4e78241-fb20-47b0-ad8b-0efd7f3f8f00"
      },
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "best = np.inf\n",
        "\n",
        "for e in range(1, epochs + 1):\n",
        "    model.train()    \n",
        "    train_loss = 0 \n",
        "    for batch in tqdm(train_loader):\n",
        "        x, y = batch.text, batch.target\n",
        "        out = model(x)\n",
        "        \n",
        "        loss = criterion(out.flatten(0, 1), y.flatten())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "    train_loss /= len(train_loader)\n",
        "    train_ppl = np.exp(train_loss)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            x, y = batch.text, batch.target\n",
        "            out = model(x)\n",
        "            loss = criterion(out.flatten(0, 1), y.flatten())\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_ppl = np.exp(valid_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    if valid_loss < best:\n",
        "        best = valid_loss\n",
        "        torch.save({'state_dict': model.state_dict()}, 'rhn.pth')\n",
        "    \n",
        "    print(\"\\nEpoch {:3d} | Train Loss {:.4f} | Valid Loss {:.4f} | Train Ppl {:.4f} | Valid Ppl {:.4f}\".format(e, train_loss, valid_loss, train_ppl, valid_ppl))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:39<00:00,  1.99it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.80it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   1 | Train Loss 6.4903 | Valid Loss 5.8666 | Train Ppl 658.7384 | Valid Ppl 353.0360\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:39<00:00,  1.91it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.21it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   2 | Train Loss 5.6708 | Valid Loss 5.3738 | Train Ppl 290.2679 | Valid Ppl 215.6906\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:39<00:00,  2.04it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.95it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   3 | Train Loss 5.3197 | Valid Loss 5.1101 | Train Ppl 204.3157 | Valid Ppl 165.6794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:38<00:00,  1.88it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.36it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   4 | Train Loss 5.1056 | Valid Loss 4.9288 | Train Ppl 164.9512 | Valid Ppl 138.2204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  2.05it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   5 | Train Loss 4.9780 | Valid Loss 4.8309 | Train Ppl 145.1893 | Valid Ppl 125.3243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:38<00:00,  2.07it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.01it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   6 | Train Loss 4.8974 | Valid Loss 4.7597 | Train Ppl 133.9452 | Valid Ppl 116.7142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.06it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   7 | Train Loss 4.8360 | Valid Loss 4.7047 | Train Ppl 125.9622 | Valid Ppl 110.4628\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.09it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.02it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   8 | Train Loss 4.7873 | Valid Loss 4.6606 | Train Ppl 119.9719 | Valid Ppl 105.6943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:37<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.08it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch   9 | Train Loss 4.7429 | Valid Loss 4.6207 | Train Ppl 114.7612 | Valid Ppl 101.5620\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.05it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.02it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  10 | Train Loss 4.7042 | Valid Loss 4.5860 | Train Ppl 110.4110 | Valid Ppl 98.1029\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.12it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  11 | Train Loss 4.6730 | Valid Loss 4.5568 | Train Ppl 107.0227 | Valid Ppl 95.2814\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:37<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.02it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  12 | Train Loss 4.6434 | Valid Loss 4.5328 | Train Ppl 103.8986 | Valid Ppl 93.0188\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  1.87it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.40it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  13 | Train Loss 4.6182 | Valid Loss 4.5083 | Train Ppl 101.3162 | Valid Ppl 90.7631\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.10it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.04it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  14 | Train Loss 4.5947 | Valid Loss 4.4870 | Train Ppl 98.9600 | Valid Ppl 88.8573\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  2.09it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.08it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  15 | Train Loss 4.5710 | Valid Loss 4.4691 | Train Ppl 96.6429 | Valid Ppl 87.2802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.06it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  16 | Train Loss 4.5515 | Valid Loss 4.4516 | Train Ppl 94.7758 | Valid Ppl 85.7660\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.08it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  17 | Train Loss 4.5324 | Valid Loss 4.4368 | Train Ppl 92.9788 | Valid Ppl 84.5015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:37<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.03it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  18 | Train Loss 4.5149 | Valid Loss 4.4202 | Train Ppl 91.3645 | Valid Ppl 83.1124\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.02it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  19 | Train Loss 4.4975 | Valid Loss 4.4108 | Train Ppl 89.7901 | Valid Ppl 82.3364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.08it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.09it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  20 | Train Loss 4.4833 | Valid Loss 4.3991 | Train Ppl 88.5307 | Valid Ppl 81.3787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  1.83it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.57it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  21 | Train Loss 4.4669 | Valid Loss 4.3860 | Train Ppl 87.0836 | Valid Ppl 80.3168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  1.93it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.40it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  22 | Train Loss 4.4540 | Valid Loss 4.3724 | Train Ppl 85.9672 | Valid Ppl 79.2351\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.10it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.06it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  23 | Train Loss 4.4413 | Valid Loss 4.3628 | Train Ppl 84.8860 | Valid Ppl 78.4778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  24 | Train Loss 4.4279 | Valid Loss 4.3557 | Train Ppl 83.7560 | Valid Ppl 77.9221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  2.10it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.10it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  25 | Train Loss 4.4169 | Valid Loss 4.3449 | Train Ppl 82.8365 | Valid Ppl 77.0872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.06it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  26 | Train Loss 4.4041 | Valid Loss 4.3357 | Train Ppl 81.7886 | Valid Ppl 76.3799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  1.88it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.39it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  27 | Train Loss 4.3942 | Valid Loss 4.3274 | Train Ppl 80.9818 | Valid Ppl 75.7475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  28 | Train Loss 4.3826 | Valid Loss 4.3165 | Train Ppl 80.0483 | Valid Ppl 74.9246\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.09it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  29 | Train Loss 4.3723 | Valid Loss 4.3095 | Train Ppl 79.2228 | Valid Ppl 74.4023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.09it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  30 | Train Loss 4.3615 | Valid Loss 4.3039 | Train Ppl 78.3774 | Valid Ppl 73.9873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  2.12it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.10it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  31 | Train Loss 4.3544 | Valid Loss 4.2953 | Train Ppl 77.8233 | Valid Ppl 73.3549\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.09it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.06it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  32 | Train Loss 4.3442 | Valid Loss 4.2917 | Train Ppl 77.0325 | Valid Ppl 73.0931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.11it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  33 | Train Loss 4.3358 | Valid Loss 4.2808 | Train Ppl 76.3894 | Valid Ppl 72.2964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  2.09it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.99it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  34 | Train Loss 4.3261 | Valid Loss 4.2754 | Train Ppl 75.6486 | Valid Ppl 71.9065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  2.05it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.05it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  35 | Train Loss 4.3168 | Valid Loss 4.2703 | Train Ppl 74.9488 | Valid Ppl 71.5404\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:35<00:00,  1.95it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.36it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  36 | Train Loss 4.3102 | Valid Loss 4.2648 | Train Ppl 74.4575 | Valid Ppl 71.1502\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  2.10it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.11it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  37 | Train Loss 4.3024 | Valid Loss 4.2583 | Train Ppl 73.8737 | Valid Ppl 70.6903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.13it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.08it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  38 | Train Loss 4.2943 | Valid Loss 4.2517 | Train Ppl 73.2774 | Valid Ppl 70.2243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:34<00:00,  2.12it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  6.08it/s]\n",
            "  0%|          | 0/170 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  39 | Train Loss 4.2854 | Valid Loss 4.2453 | Train Ppl 72.6297 | Valid Ppl 69.7795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 170/170 [01:36<00:00,  1.83it/s]\n",
            "100%|██████████| 14/14 [00:02<00:00,  5.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  40 | Train Loss 4.2782 | Valid Loss 4.2413 | Train Ppl 72.1117 | Valid Ppl 69.4991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "eLcl8ZpI4_Qu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see the loss curves."
      ]
    },
    {
      "metadata": {
        "id": "Dsdq3sbEFryG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "ecf87d98-a22d-4551-b16a-55445d8d7731"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(data={'train':train_losses, 'valid':valid_losses}).plot.line()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a99280d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEoYU7iIiYUjhLiIShhTuIiJh6P8DX9lY2edXZ5AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LCOOZtuu5BMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll load the best model and evaluate on the test set."
      ]
    },
    {
      "metadata": {
        "id": "rtcZQ5X7peQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2e23f640-7a04-4841-a791-ab14e2852c7c"
      },
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('rhn.pth')\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        x, y = batch.text, batch.target\n",
        "        out = model(x)\n",
        "        loss = criterion(out.flatten(0, 1), y.flatten())\n",
        "\n",
        "        test_loss += loss.item()\n",
        "test_loss /= len(test_loader)\n",
        "test_ppl = np.exp(test_loss)\n",
        "\n",
        "print(\"\\nTest Loss {:.4f} | Test Ppl {:.4f}\".format(test_loss, test_ppl))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:02<00:00,  5.77it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test Loss 4.0991 | Test Ppl 60.2868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ST--btlN5hUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We got a test perplexity of ~60 with a two-layer recurrent highway network with a depth of 5. The best score on the paper was ~66 with a recurrence depth of 9. Do note that in the original paper, the authors only tested single layer networks in order to study the effect of the recurrence depth, and mentioned that stacking more layers will give better test scores.\n",
        "\n",
        "This is our best score so far!\n",
        "\n",
        "Let's implement beam search and try to generate text."
      ]
    },
    {
      "metadata": {
        "id": "OzdxV8MjFi19",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This implementation deffo could use some work. I just wrote this\n",
        "# from the top of my head after a couple drinks HAHA. If you see any\n",
        "# errors, don't be afraid to reach out!\n",
        "def get_candidates(text, score, k=5):\n",
        "  s = torch.LongTensor([TEXT.vocab.stoi[w] for w in tokenize(text)]).unsqueeze(0)\n",
        "  out = model(s)\n",
        "  probs = torch.softmax(out.squeeze(0)[-1], dim=0)\n",
        "  scores, indices = torch.topk(probs, k)\n",
        "  words = [TEXT.vocab.itos[i] for i in indices]\n",
        "  checks = set(['<', '>', 'unk', '<eos>', '<sos>'])\n",
        "  candidates = [text + \" \" + c for c in words if c not in checks]\n",
        "  scores = scores * score\n",
        "  return scores, candidates\n",
        "\n",
        "def beam_search(text, n_words, k):\n",
        "  scores, cands = get_candidates(text, 1)\n",
        "  for i in tqdm(range(n_words)):\n",
        "    ncands = []\n",
        "    nscores = []\n",
        "    for s, c in zip(scores, cands):\n",
        "      ns, nc = get_candidates(c, s, k=k)\n",
        "      ncands.extend(nc)\n",
        "      nscores.extend(ns)\n",
        "    scores = []\n",
        "    cands = []\n",
        "    for c, s in sorted(list(zip(ncands, nscores)), key=lambda x: x[1], reverse=True)[:k]:\n",
        "      scores.append(s)\n",
        "      cands.append(c)\n",
        "  return cands[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B1fe2_a9LneZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's test it."
      ]
    },
    {
      "metadata": {
        "id": "lrRvf1rcISj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3f66bd21-94b2-431e-85b7-02378e653033"
      },
      "cell_type": "code",
      "source": [
        "n_words = 15\n",
        "k = 10\n",
        "text = \"the meaning of life is\"\n",
        "\n",
        "out = beam_search(text, n_words, k)\n",
        "print('\\n' + out)"
      ],
      "execution_count": 367,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 15/15 [00:01<00:00, 10.12it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "the meaning of life is expected the company said mr   but it has grown from new england said mr and\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "WqCGNqq3H-sK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}